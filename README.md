# DetecÃ§Ã£o de TransaÃ§Ãµes Fraudulentas
![CI](https://github.com/danielesantiago/FraudClassifier/actions/workflows/ci.yml/badge.svg)
![Dockerized](https://img.shields.io/badge/docker-ready-blue?logo=docker)

![image](https://github.com/danielesantiago/FraudClassifier/assets/64613885/2f879988-ada6-48f0-bdfe-b5557308899e)



## ğŸ“Œ Overview
O projeto visa conduzir uma anÃ¡lise exploratÃ³ria dos dados e construir modelos de machine learning para detectar transaÃ§Ãµes fraudulentas com alta precisÃ£o. Utilizamos tÃ©cnicas avanÃ§adas de anÃ¡lise de dados, machine learning e balanceamento de dados para identificar padrÃµes e anomalias.

ğŸ“„ [Veja a minha apresentaÃ§Ã£o aqui](https://github.com/danielesantiago/FraudClassifier/blob/master/reports/Apresenta%C3%A7%C3%A3o%20Fraude.pdf)


## ğŸ’¼ Business Understanding

A fraude em transaÃ§Ãµes representa uma ameaÃ§a crescente. Ã€ medida que o uso de cartÃ£o de crÃ©dito aumenta, as fraudes tambÃ©m acompanham esse crescimento. Entender essa ameaÃ§a e suas mÃ©tricas Ã© fundamental para combater efetivamente tais atividades. 

**Tipos de Fraude:**
- Fraude de CartÃ£o de CrÃ©dito
  - Offline
  - Online
- Fraude em TelecomunicaÃ§Ãµes
- Fraude em Computadores
- Outros Tipos (Fraude de FalÃªncia, Fraude de Furto/ContrafaÃ§Ã£o, etc.)

**Principais KPIs (Indicadores Chave de Desempenho) de Fraude:**
- AceitaÃ§Ã£o
- Desafios
- NegaÃ§Ãµes
- Chargebacks
- Falsos Positivos

## ğŸ“Š AnÃ¡lise do Modelo Atual

A eficÃ¡cia do nosso modelo de detecÃ§Ã£o de fraude Ã© vital para a saÃºde financeira da empresa. A empresa ganha 10% do valor de um pagamento corretamente aprovado, mas sofre uma perda completa, ou seja, 100%, em caso de uma transaÃ§Ã£o fraudulenta. Portanto, otimizar a Taxa de Fraude e a Taxa de AprovaÃ§Ã£o Ã© fundamental.

Os dados atuais apontam para algumas Ã¡reas de preocupaÃ§Ã£o:

![image](https://github.com/danielesantiago/FraudClassifier/assets/64613885/4887e641-a6d6-4256-bdc7-3ea26a7849d2)


Ao observar o grÃ¡fico, notamos um problema significativo: as classes estÃ£o notavelmente sobrepostas. Isso indica que nosso modelo tem dificuldades em distinguir entre transaÃ§Ãµes legÃ­timas e fraudulentas. Idealmente, gostarÃ­amos de ver uma separaÃ§Ã£o mais clara entre as duas classes, o que indicaria que o modelo pode identificar caracterÃ­sticas distintas associadas a cada tipo de transaÃ§Ã£o. A sobreposiÃ§Ã£o sugere que muitas transaÃ§Ãµes legÃ­timas e fraudulentas tÃªm caracterÃ­sticas semelhantes, tornando a tarefa de classificaÃ§Ã£o mais desafiadora.

- **Financeiro**:
  - **Perdas com fraudes**: $25,353.320
  - **Receitas**: $80,329.995
  - **Lucro lÃ­quido**: $54,976.675
  - **RazÃ£o de lucro**: 68.44%.

- **Desempenho do Modelo**:
  - **Taxa de Fraude**: 2%
  - **Taxa de AprovaÃ§Ã£o**: 74%
  - **Log Loss**: 8.2526
  - **ROC-AUC**: 0.7193
 
A falta de distinÃ§Ã£o clara entre as classes, como ilustrado na imagem, combinada com as mÃ©tricas de desempenho fornecidas, sugere a necessidade de uma revisÃ£o e possivelmente uma reformulaÃ§Ã£o do modelo. Isso pode envolver considerar outras features, aplicar tÃ©cnicas de balanceamento de classes ou experimentar diferentes algoritmos de classificaÃ§Ã£o.


## ğŸ›  PrÃ©-processamento 
O prÃ©-processamento de dados Ã© uma etapa crucial em projetos de machine learning, e para garantir a eficÃ¡cia e reprodutibilidade do nosso processo, utilizamos o **Pipeline do Scikit-learn**. O uso de um pipeline assegura que as transformaÃ§Ãµes aplicadas aos dados de treinamento sejam reproduzidas de forma idÃªntica nos dados de teste, eliminando potenciais erros e inconsistÃªncias.

_ConsideraÃ§Ãµes_:
1. A coluna valor_compra refere-se ao valor da compra e estÃ¡ em uma Ãºnica unidade (ex: DÃ³lar).
2. NÃ£o hÃ¡ custo extra de fraude alÃ©m do mencionado.
3. Nenhuma das colunas inseridas no modelo causarÃ¡ data leakage; ou seja, todos esses dados sÃ£o calculados/recebidos antes que o evento "Fraude" ocorra.
   
_Etapas do PrÃ©-processamento no Pipeline_:
1. ExclusÃ£o de Colunas:
* score_fraude_modelo: Modelo baseline que nÃ£o deve ser utilizado.
* data_compra: Para prevenir a degradaÃ§Ã£o do modelo com o tempo.
* produto: Devido a alta cardinalidade (mais de 8 mil categorias).

2. Tratamento de Categorias:
* Manter as 1000 categorias em categoria_produto que correspondem a 80% das fraudes.
* Limitar paÃ­s para "BR", "AR" (que compÃµem mais de 90% da distribuiÃ§Ã£o) e "outros".
* Target encoding em categoria_produto devido a alta cardinalidade.
* One hot encoding nas demais variÃ¡veis categÃ³ricas.

3. Tratamento de Valores Nulos:
* Preencher os nulos de score com a mediana, visto que nÃ£o seguem uma distribuiÃ§Ã£o normal.
* Criar uma feature is_null indicando quais valores de entrega_doc_2 sÃ£o nulos.
* Considerar os nulos de entrega_doc_2 como 0, indicando "nÃ£o entregue".
  
## ğŸ¤– MLFlow

MLFlow Ã© uma plataforma aberta de gerenciamento do ciclo de vida de machine learning. Ele oferece ferramentas para rastrear experimentos, empacotar cÃ³digo em formatos reprodutÃ­veis e compartilhar e implantar modelos. Uma das principais vantagens do MLFlow Ã© a sua capacidade de registrar mÃ©tricas, parÃ¢metros e artefatos, facilitando a comparaÃ§Ã£o entre diferentes versÃµes de modelos e a reproduÃ§Ã£o de experimentos.

Para o nosso projeto, utilizamos o MLFlow como ferramenta de rastreamento. As mÃ©tricas priorizadas para avaliaÃ§Ã£o e comparaÃ§Ã£o dos modelos foram:
- Taxa de aprovaÃ§Ã£o
- Lucro gerado pelo modelo atual
- ROC-AUC (Ã¡rea sob a curva caracterÃ­stica de operaÃ§Ã£o do receptor)
- RazÃ£o de lucro
  
Dessa maneira, foi possÃ­vel monitorar e otimizar o desempenho do nosso modelo de forma eficiente, garantindo resultados mais robustos e transparentes. A visualizaÃ§Ã£o das mÃ©tricas e experimentos do projeto podem ser visualizadas no dashboard MLFlow:

![image](https://github.com/danielesantiago/FraudClassifier/assets/64613885/d4424e41-4153-4331-90b1-27266dc8c965)



## ğŸ“ˆ Modelo Treinado

Ao avaliar o desempenho do Modelo Atual em comparaÃ§Ã£o com o Modelo Treinado, Ã© evidente que o Ãºltimo apresenta melhorias significativas nÃ£o apenas em mÃ©tricas de desempenho, mas tambÃ©m no impacto financeiro.

![image](https://github.com/danielesantiago/FraudClassifier/assets/64613885/b5a946df-0d58-4756-965a-cbaccaae2c5a)


**AnÃ¡lise Visual da Imagem**:
Ao examinar o grÃ¡fico, vemos uma distinÃ§Ã£o mais clara entre as transaÃ§Ãµes legÃ­timas e as fraudulentas no Modelo Treinado. Essa separaÃ§Ã£o melhor definida sugere que o modelo Ã© mais capaz de identificar as caracterÃ­sticas distintivas das transaÃ§Ãµes, resultando em classificaÃ§Ãµes mais precisas.

**Desempenho Financeiro e MÃ©tricas**:
- **Threshold Ã³timo**: 61
- **Perdas com fraudes**: $27,007.200
- **Receitas**: $95,198.968
- **Lucro lÃ­quido**: $68,191.768
- **ROC-AUC**: 0.8512
- **Taxa de Fraude**: 0.02
- **Taxa de AprovaÃ§Ã£o**: 0.85
- **RazÃ£o Lucro/Receitas**: 72%

**ComparaÃ§Ã£o entre Modelo Atual e Modelo Treinado**:

- **Taxa de fraude**: MantÃ©m-se constante em 0.02 para ambos os modelos.
- **Taxa de aprovaÃ§Ã£o**: O Modelo Treinado tem uma taxa de aprovaÃ§Ã£o superior, 0.85 em comparaÃ§Ã£o com 0.74 do Modelo Atual.
- **RazÃ£o Lucro/Receitas**: O Modelo Treinado mostra uma melhoria de 4%, passando de 68% no Modelo Atual para 72%.

## ğŸ“Š AnÃ¡lise ExploratÃ³ria, SHAP e Testes de HipÃ³teses

Para aprofundar nosso entendimento sobre o comportamento do modelo, conduzimos uma anÃ¡lise exploratÃ³ria detalhada, complementada pela anÃ¡lise SHAP. Essa anÃ¡lise SHAP nos permitiu destrinchar a relevÃ¢ncia de cada variÃ¡vel e entender seu impacto nas previsÃµes. Adicionalmente, realizamos testes de hipÃ³teses para validar e solidificar nossas descobertas, garantindo que as observaÃ§Ãµes sÃ£o estatisticamente significativas. O detalhamento dessas anÃ¡lises pode ser acessado em nosso repositÃ³rio: 
[Case Fraude no GitHub](https://github.com/danielesantiago/FraudClassifier/blob/master/notebooks/Case%20Fraude.ipynb).


## ğŸ“œ Estrutura do Projeto

A estrutura de diretÃ³rios do projeto foi organizada da seguinte forma:
```
â”œâ”€â”€ README.md 
â”œâ”€â”€ data
â”‚ â”œâ”€â”€ processed
â”‚ â””â”€â”€ raw
â”œâ”€â”€ models
â”‚ â””â”€â”€ model_pipeline.pkl
â”œâ”€â”€ notebooks 
â”œâ”€â”€ reports
â”‚ â””â”€â”€ figures 
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src
â”‚ â”œâ”€â”€ init.py 
â”‚ â”œâ”€â”€ features.py
â”‚ â”œâ”€â”€ config.py
â”‚ â”œâ”€â”€ models
â”‚ â”‚ â”œâ”€â”€ predict_model.py 
â”‚ â”‚ â””â”€â”€ train_model.py
â”œâ”€â”€ tests
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ test_features.py
â”‚ â”œâ”€â”€ test_predict.py 

```

## âš™ï¸ IntegraÃ§Ã£o ContÃ­nua com GitHub Actions

Este projeto utiliza **CI (IntegraÃ§Ã£o ContÃ­nua)** via GitHub Actions para garantir a qualidade do cÃ³digo e facilitar a colaboraÃ§Ã£o.

A pipeline Ã© acionada a cada push ou pull request na branch `master` e executa as seguintes etapas:

1. âœ… Checkout do cÃ³digo
2. ğŸ InstalaÃ§Ã£o do Python 3.12
3. ğŸ“¦ InstalaÃ§Ã£o de dependÃªncias com Poetry
4. ğŸ¨ VerificaÃ§Ã£o de formataÃ§Ã£o com **Black** na pasta src
5. ğŸ§¹ AnÃ¡lise estÃ¡tica com **Pylint** (mÃ­nimo 8.0) para o cÃ³digo do modelo
6. âœ… ExecuÃ§Ã£o de **Pytest** para os testes automatizados


## ğŸ³ Deploy com Docker

Para facilitar o deploy local da API de prediÃ§Ã£o de fraudes, este projeto conta com um ambiente containerizado via Docker. Com isso, Ã© possÃ­vel executar a aplicaÃ§Ã£o em qualquer mÃ¡quina com Docker instalado, sem necessidade de configurar o ambiente manualmente.

### ğŸ”§ Construir a imagem

```bash
docker build -t fraud-api .
```

### ğŸš€ Rodar a API

```bash
docker run -p 8000:8000 fraud-api
```

ApÃ³s iniciar o container, acesse a documentaÃ§Ã£o interativa da API:

```
http://localhost:8000/docs
```

VocÃª tambÃ©m pode utilizar o `Makefile` para facilitar a execuÃ§Ã£o dos comandos:

```bash
make build      # Build da imagem Docker
make run        # Executa a API
make test-payload  # Envia payload de teste
```

A API foi desenvolvida com FastAPI e carrega um pipeline de machine learning previamente treinado (`model_pipeline.pkl`), pronto para realizar inferÃªncias em tempo real.


### ğŸ“š Como testar a API

Acesse a documentaÃ§Ã£o interativa da API em:

```
http://localhost:8000/docs
```

Na documentaÃ§Ã£o, vocÃª poderÃ¡ visualizar os endpoints disponÃ­veis e testar as prediÃ§Ãµes diretamente pela interface do Swagger UI.

### ğŸš€ Testando a prediÃ§Ã£o via **`curl`**:

Para testar a prediÃ§Ã£o da API de forma prÃ¡tica, utilize o comando `curl` para enviar um payload com dados de exemplo para o endpoint `/predict`.

1. **Use o arquivo `test_payload.json`** com os dados de exemplo para envio.
2. **Execute o seguinte comando**:

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d @test_payload.json
```

Este comando envia uma solicitaÃ§Ã£o POST para o endpoint `/predict`, com o conteÃºdo do arquivo `test_payload.json` contendo as informaÃ§Ãµes de entrada para a prediÃ§Ã£o.

### ğŸ“ˆ O que esperar da resposta:

A resposta retornada pela API conterÃ¡ as prediÃ§Ãµes feitas pelo modelo de machine learning, com a **probabilidade de fraude** associada a cada transaÃ§Ã£o. O formato da resposta serÃ¡ algo como:

```json
{
  "results": [
    {
      "prediction": 0,
      "probability": 0.25
    },
    {
      "prediction": 1,
      "probability": 0.85
    }
  ]
}
```

### ğŸ¬ DemonstraÃ§Ã£o

Veja abaixo um exemplo da API em funcionamento:

![DemonstraÃ§Ã£o da API](https://raw.githubusercontent.com/danielesantiago/FraudClassifier/refs/heads/master/reports/figures/ezgif-200991689f5bff.gif)



## ğŸ“Š Dashboard de Monitoramento de Fraudes

Este projeto conta com um dashboard interativo desenvolvido com **Streamlit**, que permite **analisar, visualizar e monitorar o desempenho de um modelo de detecÃ§Ã£o de fraudes** ao longo do tempo.

O dashboard estÃ¡ dividido em duas seÃ§Ãµes principais:

### ğŸ“Š GrÃ¡ficos
- VisualizaÃ§Ã£o dos **scores de fraude** previstos pelo modelo
- CÃ¡lculo de **mÃ©tricas de classificaÃ§Ã£o** (Accuracy, Precision, Recall, F1-score)
- Matrizes de confusÃ£o (absoluta e proporcional)
- MÃ©tricas operacionais como **taxa de aprovaÃ§Ã£o** e **fraude aprovada**
- Estimativas financeiras de lucro, perda e receita com base nas decisÃµes do modelo

### ğŸ“¡ Monitoramento
- ComparaÃ§Ãµes entre os dados de treino e de produÃ§Ã£o
- Acompanhamento de **mÃ©tricas de performance estimadas ao longo do tempo** com o uso do NannyML
- Monitoramento da **qualidade dos dados**, incluindo:
  - Quantidade de valores nulos
  - Valores categÃ³ricos nÃ£o observados anteriormente
  - AlteraÃ§Ãµes nas distribuiÃ§Ãµes estatÃ­sticas

> ğŸ“ˆ **Com essas informaÃ§Ãµes, Ã© possÃ­vel monitorar mÃ©tricas de negÃ³cio e detectar sinais de _drift_ (mudanÃ§as nos dados). Isso permite tomar decisÃµes mais assertivas, como o momento ideal para re-treinar o modelo e garantir sua performance ao longo do tempo.**

### ğŸ¬ DemonstraÃ§Ã£o

Veja abaixo o dashboard em funcionamento:

![DemonstraÃ§Ã£o do Dashboard](https://raw.githubusercontent.com/danielesantiago/FraudClassifier/refs/heads/master/reports/figures/ezgif-2bec018b49a54b.gif)



**ObservaÃ§Ã£o sobre os Dados:**

Os dados utilizados neste notebook pertencem ao PreparatÃ³rio para Entrevistas em Dados (PED). Por motivos de privacidade e restriÃ§Ãµes de compartilhamento, esses dados nÃ£o estÃ£o incluÃ­dos diretamente no notebook.

**Como Acessar os Dados:**

Se vocÃª estiver interessado em realizar este estudo de caso e necessitar dos dados, eles estÃ£o disponÃ­veis no seguinte link: [PreparatÃ³rio para Entrevistas em Dados (PED)](https://www.renatabiaggi.com/ped).

Neste link, vocÃª encontrarÃ¡ todas as informaÃ§Ãµes necessÃ¡rias para acessar e utilizar os dados para fins de anÃ¡lise e pesquisa.
